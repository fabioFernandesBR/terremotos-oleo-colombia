{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568121d6-27fe-46c2-ae45-2ab409be8e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Camada Ouro - Notebook único - Criação do Esquema SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e851160-d037-4951-a59c-64bd200fbaf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### importes necessários\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9b71bf-f263-4dd2-8dbe-083a3e37a16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Passo 1 - Importar os dataframes da camada prata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa54126d-a3b3-45a6-8b7a-8775ad4e7374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- prop_mag: double (nullable = true)\n |-- prop_url: string (nullable = true)\n |-- data: date (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- pais: string (nullable = true)\n\n+----------+--------+--------------------+----------+--------+---------+------------+\n|        id|prop_mag|            prop_url|      data|latitude|longitude|        pais|\n+----------+--------+--------------------+----------+--------+---------+------------+\n|at00sthakw|     6.5|https://earthquak...|2025-03-21|     7.2|    -82.3|Desconhecido|\n|us10001ldx|     6.2|https://earthquak...|2015-03-10|  6.7757| -72.9875|    Colombia|\n|us10001pn4|     5.1|https://earthquak...|2015-03-22|   6.804|  -73.147|    Colombia|\n|us10001rcr|     5.5|https://earthquak...|2015-03-27| -1.2012| -77.5836|     Ecuador|\n|us10002azw|     5.1|https://earthquak...|2015-05-20| -3.0826| -77.5501|        Perú|\n|us100030j8|     5.4|https://earthquak...|2015-08-09|    5.18| -82.6431|Desconhecido|\n|us10003mxh|     5.2|https://earthquak...|2015-10-12|  7.5362|  -77.476|    Colombia|\n|us10003niy|     5.0|https://earthquak...|2015-10-14|  7.6522| -73.3038|    Colombia|\n|us10003nud|     5.4|https://earthquak...|2015-10-15| -2.5015| -78.7623|     Ecuador|\n|us10003vgf|     5.3|https://earthquak...|2015-11-07|  8.4672| -71.3957|   Venezuela|\n+----------+--------+--------------------+----------+--------+---------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo Delta Lake\n",
    "file_path = \"/mnt/dados/camada_prata_terremotos\"\n",
    "\n",
    "# Ler o arquivo Delta Lake\n",
    "df_terremotos = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "df_terremotos.printSchema()\n",
    "df_terremotos.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87174129-14ea-4bd3-bcec-b6cd7fde6f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- codigo: string (nullable = true)\n |-- instalacao: string (nullable = true)\n |-- operador: string (nullable = true)\n |-- latitude_instalacao: double (nullable = true)\n |-- longitude_instalacao: double (nullable = true)\n\n+------+--------------------+--------+-------------------+--------------------+\n|codigo|          instalacao|operador|latitude_instalacao|longitude_instalacao|\n+------+--------------------+--------+-------------------+--------------------+\n|   COV|    Terminal Coveñas|  Ocensa|  9.409136280745889|  -75.70015297027714|\n|   LAG|Estacion La Granjita|  Ocensa|  8.449501151686118|  -75.50994443266501|\n|   CAU|     Planta Caucasia|     ODC|  7.976933967792115|  -75.22089861123531|\n+------+--------------------+--------+-------------------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo Delta Lake\n",
    "file_path = \"/mnt/dados/camada_prata_instalacoes\"\n",
    "\n",
    "# Ler o arquivo Delta Lake\n",
    "df_instalacoes = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "df_instalacoes.printSchema()\n",
    "df_instalacoes.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da72be69-07d4-4f95-a085-5ac5028bf5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- instalacoes_cod: string (nullable = true)\n |-- terremotos_id: string (nullable = true)\n |-- distancia: double (nullable = true)\n\n+---------------+-------------+------------------+\n|instalacoes_cod|terremotos_id|         distancia|\n+---------------+-------------+------------------+\n|            CHI|   us10001ldx| 165.8097685759375|\n|            VAS|   us10001ldx|190.64982815656515|\n|            LAB|   us10001ldx| 149.7729764777793|\n+---------------+-------------+------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo Delta Lake\n",
    "file_path = \"/mnt/dados/camada_prata_impacto\"\n",
    "\n",
    "# Ler o arquivo Delta Lake\n",
    "df_impacto = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "df_impacto.printSchema()\n",
    "df_impacto.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9af69e4-ac0c-4385-ae27-cd4b54989ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Passo 2 - Criar o esquema SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4c2b13-1cbf-44e3-b290-ddd803520e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Infelizmente, na versão Community Edition, o Databricks não oferece a possibilidade de criação de databases SQL com garantias de integridade referencial, como uso de chaves primárias e estrangeiras._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e488d430-e2d1-41f1-8c35-c97fc15d1bff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: True"
     ]
    }
   ],
   "source": [
    "\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/impactos', recurse=True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/instalacoes', recurse=True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/terremotos', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2adc5447-5932-4b01-811b-169409b2a689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS impactos;\n",
    "DROP TABLE IF EXISTS instalacoes;\n",
    "DROP TABLE IF EXISTS terremotos;\n",
    "\n",
    "\n",
    "CREATE TABLE impactos (\n",
    "  instalacoes_cod VARCHAR(8),\n",
    "  terremotos_id VARCHAR(255),\n",
    "  distancia DOUBLE\n",
    ");\n",
    "\n",
    "CREATE TABLE instalacoes (\n",
    "  codigo VARCHAR(8),\n",
    "  instalacao VARCHAR(255),\n",
    "  operador VARCHAR(255),\n",
    "  latitude_instalacao DOUBLE,\n",
    "  longitude_instalacao DOUBLE\n",
    ");\n",
    "\n",
    "CREATE TABLE terremotos (\n",
    "  id VARCHAR(255),\n",
    "  prop_mag DOUBLE,\n",
    "  prop_url VARCHAR(1024),\n",
    "  data DATE,\n",
    "  latitude DOUBLE,\n",
    "  longitude DOUBLE,\n",
    "  pais VARCHAR(255)\n",
    ");\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421ee966-e53b-4730-b8d3-42f5e6b2ef83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Passo 3 - Mover dados dos dataframes para o esquema SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9203caac-c111-4392-aa81-051bda40c8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escrevendo o DataFrame df_impactos na tabela impactos\n",
    "df_impacto.write.mode(\"overwrite\").saveAsTable(\"impactos\")\n",
    "\n",
    "# Escrevendo o DataFrame df_instalacoes na tabela instalacoes\n",
    "df_instalacoes.write.mode(\"overwrite\").saveAsTable(\"instalacoes\")\n",
    "\n",
    "# Escrevendo o DataFrame df_terremotos na tabela terremotos\n",
    "df_terremotos.write.mode(\"overwrite\").saveAsTable(\"terremotos\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2116622501217693,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Camada Ouro - Notebook único - Esquema SQL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}